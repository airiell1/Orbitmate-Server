const axios = require("axios");
const config = require("./index"); // ì¤‘ì•™ ì„¤ì • íŒŒì¼ import

const OLLAMA_API_URL = config.ai.ollama.apiUrl;
const DEFAULT_OLLAMA_MODEL = config.ai.ollama.defaultModel;

if (!OLLAMA_API_URL && config.ai.defaultProvider === 'ollama') {
  // ê¸°ë³¸ ì œê³µìê°€ ollamaì¼ ë•Œë§Œ ì—ëŸ¬ ì¶œë ¥, ì•„ë‹ˆë©´ ê²½ê³ 
  const message = "OLLAMA_API_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Ollama APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
   if (config.ai.defaultProvider === 'ollama') {
    console.error(message);
  } else {
    console.warn(message);
  }
}

/**
 * ë¡œì»¬ Ollama ëª¨ë¸ì— ëŒ€í™” ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
 * @param {string} modelName ì‚¬ìš©í•  Ollama ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gemma3:4b"). ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©.
 * @param {string} currentUserMessage í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
 * @param {Array<{role: string, parts: Array<{text: string}>}>} history Vertex AI í˜•ì‹ì˜ ëŒ€í™” ê¸°ë¡
 * @param {string | null} systemMessageText ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€
 * @param {function(string): void} [streamResponseCallback] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¨ìˆ˜
 * @param {Object} options ì¶”ê°€ ì˜µì…˜ (í˜„ì¬ Ollamaì—ì„œëŠ” íŠ¹ë³„íˆ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, í–¥í›„ í™•ì¥ ê°€ëŠ¥)
 * @returns {Promise<{content: string, model_used: string, provider: string} | null>} AI ì‘ë‹µ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ null
 */
async function getOllamaResponse(
  modelName, // modelNameì´ ì²« ë²ˆì§¸ ì¸ìë¡œ ìœ ì§€ (aiProvider.js í˜¸ì¶œ ì‹œê·¸ë‹ˆì²˜ì™€ ì¼ê´€ì„±)
  currentUserMessage,
  history = [],
  systemMessageText = null,
  streamResponseCallback = null,
  options = {} // options ì¸ì ì¶”ê°€
) {
  const messages = [];
  const effectiveModelName = modelName || DEFAULT_OLLAMA_MODEL; // modelNameì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©

  if (systemMessageText && systemMessageText.trim() !== "") {
    messages.push({ role: "system", content: systemMessageText });
  }

  history.forEach((entry) => {
    if (entry.parts && entry.parts.length > 0 && entry.parts[0].text) {
      messages.push({
        role: entry.role === "model" ? "assistant" : "user",
        content: entry.parts[0].text,
      });
    }
  });

  // ğŸ”§ ì¤‘ë³µ ë°©ì§€: ëŒ€í™” ì´ë ¥ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ê°™ìœ¼ë©´ ëª¨ë‘ ì œê±°
  while (messages.length > 0) {
    const lastMsg = messages[messages.length - 1];
    if (lastMsg.role === "user" && lastMsg.content === currentUserMessage) {
      console.log(`[Ollama] ì¤‘ë³µëœ ì‚¬ìš©ì ë©”ì‹œì§€ ë°œê²¬, ì´ë ¥ì—ì„œ ì œê±°: "${currentUserMessage.substring(0, 50)}..."`);
      messages.pop(); // ë§ˆì§€ë§‰ ì¤‘ë³µ ë©”ì‹œì§€ ì œê±°
    } else {
      break; // ì¤‘ë³µì´ ì•„ë‹ˆë©´ ë£¨í”„ ì¢…ë£Œ
    }
  }

  messages.push({ role: "user", content: currentUserMessage });

  if (!OLLAMA_API_URL) {
    const errorMsg = "Ollama API URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ìš”ì²­ì„ ë³´ë‚¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
    console.error(`[Ollama] ${errorMsg}`);
    // ìŠ¤íŠ¸ë¦¬ë° ì½œë°±ì´ ìˆìœ¼ë©´ ì˜¤ë¥˜ë¥¼ ì „ë‹¬í•˜ê³ , ì•„ë‹ˆë©´ ì—ëŸ¬ ê°ì²´ë¥¼ í¬í•¨í•œ ì‘ë‹µ ë°˜í™˜
    if (streamResponseCallback) {
        streamResponseCallback(null, new Error(errorMsg)); // ìŠ¤íŠ¸ë¦¬ë° ì½œë°±ì— ì—ëŸ¬ ì „ë‹¬ ë°©ì‹ í†µì¼ í•„ìš”
        return null; // ë˜ëŠ” Promise.reject(new Error(errorMsg));
    }
    return { content: `ì˜¤ë¥˜: ${errorMsg}`, model_used: effectiveModelName, provider: "ollama" };
  }

  try {
    console.log(
      `[Ollama] Requesting API with model: ${effectiveModelName}, streaming: ${!!streamResponseCallback}`
    );

    const requestPayload = {
      model: effectiveModelName,
      messages: messages,
      stream: !!streamResponseCallback,
      // options.max_output_tokens ë“± Ollamaê°€ ì§€ì›í•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
      // ì˜ˆ: options: { num_predict: options.max_output_tokens_override || -1 } // -1ì€ ë¬´ì œí•œ
    };
     if (options.max_output_tokens_override && Number.isInteger(options.max_output_tokens_override)) {
      requestPayload.options = requestPayload.options || {};
      requestPayload.options.num_predict = options.max_output_tokens_override;
    }


    const response = await axios.post(OLLAMA_API_URL, requestPayload, {
      responseType: streamResponseCallback ? "stream" : "json",
    });

    if (streamResponseCallback) {
      let accumulatedContent = "";
      return new Promise((resolve, reject) => {
        response.data.on("data", (chunk) => {
          try {
            const chunkStr = chunk.toString();
            chunkStr.split("\n").forEach((line) => {
              if (line.trim()) {
                const parsedLine = JSON.parse(line);
                if (parsedLine.message && parsedLine.message.content) {
                  const contentPart = parsedLine.message.content;
                  accumulatedContent += contentPart;
                  streamResponseCallback(contentPart);
                }
                // ìŠ¤íŠ¸ë¦¬ë° ì¤‘ í† í° ìˆ˜ ë“±ì˜ ë©”íƒ€ë°ì´í„°ëŠ” parsedLine.total_duration ë“±ìœ¼ë¡œ ì œê³µë  ìˆ˜ ìˆìŒ
                if (parsedLine.done) {
                    // ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì‹œ, ëˆ„ì ëœ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¡œ resolve
                    // console.log('[Ollama] Stream finished. Accumulated:', accumulatedContent);
                    // resolve({ content: accumulatedContent, model_used: effectiveModelName, provider: "ollama" });
                }
              }
            });
          } catch (e) {
            console.error("[Ollama] Error parsing stream chunk:", e, chunk.toString());
            // ìŠ¤íŠ¸ë¦¼ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ, í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë‚´ìš©ìœ¼ë¡œ resolve í•˜ê±°ë‚˜ reject í•  ìˆ˜ ìˆìŒ
            // ì—¬ê¸°ì„œëŠ” rejectí•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰ (ë¶€ë¶„ì ì¸ ë°ì´í„°ë¼ë„ ì „ë‹¬ ì‹œë„)
          }
        });

        response.data.on("end", () => {
          // console.log('[Ollama] Stream ended. Final accumulated:', accumulatedContent);
          // ìŠ¤íŠ¸ë¦¼ì´ ì •ìƒì ìœ¼ë¡œ 'end'ë˜ë©´, ëˆ„ì ëœ ì „ì²´ ë‚´ìš©ì„ resolve
           resolve({ content: accumulatedContent, model_used: effectiveModelName, provider: "ollama", streaming: true });
        });

        response.data.on("error", (err) => {
          console.error("[Ollama] Error during stream:", err);
          reject(err); // ìŠ¤íŠ¸ë¦¼ ìì²´ì—ì„œ ì—ëŸ¬ ë°œìƒ ì‹œ reject
        });
      });
    } else {
      // Non-streaming response
      if (response.data && response.data.message && response.data.message.content) {
        return {
          content: response.data.message.content,
          // ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œë„ í† í° ìˆ˜ ë“±ì˜ ë©”íƒ€ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶”ê°€
          // ì˜ˆ: actual_input_tokens: response.data.prompt_eval_count,
          //     actual_output_tokens: response.data.eval_count,
          model_used: effectiveModelName,
          provider: "ollama",
        };
      } else {
        console.error("[Ollama] Invalid non-streaming response structure:", response.data);
        return { content: "Ollamaë¡œë¶€í„° ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", model_used: effectiveModelName, provider: "ollama" };
      }
    }
  } catch (error) {
    const errorData = error.response ? (error.response.data || error.message) : error.message;
    console.error("[Ollama] Error calling API:", errorData);
    const errorMessage = (error.response && error.response.data && error.response.data.error)
      ? error.response.data.error
      : "Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";

    if (streamResponseCallback) {
        streamResponseCallback(null, new Error(errorMessage));
        return null;
    }
    return { content: `ì˜¤ë¥˜: ${errorMessage}`, model_used: effectiveModelName, provider: "ollama" };
  }
}

module.exports = { getOllamaResponse };
