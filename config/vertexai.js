const { VertexAI } = require("@google-cloud/vertexai");
const config = require("./index"); // ì¤‘ì•™ ì„¤ì • íŒŒì¼ import
const { logError } = require("../utils/errorHandler");

// Vertex AI ì„¤ì • (ì¤‘ì•™ ì„¤ì • íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
let vertex_ai, generativeModel;
const project = config.ai.vertexAi.projectId;
const keyFilename = config.ai.vertexAi.applicationCredentials;
const location = config.ai.vertexAi.location;
const defaultModelId = config.ai.vertexAi.defaultModel;

// ê¸°ë³¸ generationConfig, í•„ìš”ì‹œ ì¤‘ì•™ ì„¤ì •ìœ¼ë¡œ ì´ë™ ê°€ëŠ¥
const defaultGenerationConfig = {
  temperature: 0.8,
  topP: 0.95,
  maxOutputTokens: 65535, // Vertex AIì˜ ê¸°ë³¸ ìµœëŒ€ê°’ ë˜ëŠ” ì„œë¹„ìŠ¤ í•œë„ì— ë”°ë¦„
};

// ì•ˆì „ì„± ì„¤ì • (ì™„í™”ëœ ê²€ì—´ ì„¤ì •)
const safetySettings = [
  {
    category: "HARM_CATEGORY_HATE_SPEECH",
    threshold: "BLOCK_ONLY_HIGH",
  },
  {
    category: "HARM_CATEGORY_DANGEROUS_CONTENT",
    threshold: "BLOCK_ONLY_HIGH",
  },
  {
    category: "HARM_CATEGORY_HARASSMENT",
    threshold: "BLOCK_ONLY_HIGH",
  },
  {
    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
    threshold: "BLOCK_ONLY_HIGH",
  },
];

if (project && keyFilename) {
  try {
    vertex_ai = new VertexAI({ project, location, keyFilename });

    generativeModel = vertex_ai.getGenerativeModel({
      model: defaultModelId,
      generationConfig: defaultGenerationConfig,
      safetySettings: safetySettings,
    });
  } catch (error) {
    logError(
      "vertexAiConfig:startup",
      new Error(`Failed to initialize Vertex AI client or model '${defaultModelId}' during startup: ${error.message}`), // Error ê°ì²´ë¡œ ì „ë‹¬
      { originalError: error } // ìƒì„¸ ì˜¤ë¥˜ëŠ” contextë¡œ
    );
  }
} else {
   const message = "Vertex AI credentials (GOOGLE_PROJECT_ID, GOOGLE_APPLICATION_CREDENTIALS) are not set. Vertex AI will not be available.";
   if (config.ai.defaultProvider === 'vertexai') {
    console.error(`[VertexAI] ${message}`);
   } else {
    console.warn(`[VertexAI] ${message}`);
   }
}

// AI ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (Vertex AI ì „ìš©)
async function getVertexAiApiResponse(
  currentUserMessage,
  history = [],
  systemMessageText = null,
  specialModeType = null,
  streamResponseCallback = null,
  options = {} // options ê°ì²´ì— model_id_override, generationConfig, max_output_tokens_override í¬í•¨ ê°€ëŠ¥
) {
  console.log(`[VertexAI] Executing Vertex AI (Gemini) logic.`);

  if (!vertex_ai) {
    const errorMsg = "Vertex AI client not initialized. Check credentials and logs.";
    logError("getVertexAiApiResponse:init", new Error(errorMsg));
    if (streamResponseCallback && typeof streamResponseCallback === "function") {
      streamResponseCallback(null, new Error(errorMsg));
      return null;
    }
    throw new Error(errorMsg);
  }

  const modelIdToUse = options.model_id_override || defaultModelId;
  let currentGenerativeModel = generativeModel; // ê¸°ë³¸ì ìœ¼ë¡œ ì‹œìž‘ ì‹œ ì´ˆê¸°í™”ëœ ëª¨ë¸ ì‚¬ìš©

  // ëª¨ë¸ IDê°€ ë‹¤ë¥´ê±°ë‚˜, í˜„ìž¬ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° (ì‹œìž‘ ì‹œ ì‹¤íŒ¨) ë™ì ìœ¼ë¡œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
  if (modelIdToUse !== defaultModelId || !currentGenerativeModel) {
    console.log(`[VertexAI] Using dynamically configured model: ${modelIdToUse}`);
    try {
      currentGenerativeModel = vertex_ai.getGenerativeModel({
        model: modelIdToUse,
        generationConfig: { // options.generationConfigê°€ ìžˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
          ...defaultGenerationConfig,
          ...(options.generationConfig || {}),
        },
      });
    } catch (error) {
      const errorMsg = `Failed to initialize overridden Vertex AI model '${modelIdToUse}': ${error.message}`;
      logError("getVertexAiApiResponse:modelInit", new Error(errorMsg), { originalError: error });
      if (streamResponseCallback && typeof streamResponseCallback === "function") {
        streamResponseCallback(null, new Error(errorMsg));
        return null;
      }
      throw new Error(errorMsg);
    }
  } else if (options.generationConfig) { // ê°™ì€ ëª¨ë¸ì´ì§€ë§Œ generationConfigë§Œ ë³€ê²½í•˜ëŠ” ê²½ìš°
     console.log(`[VertexAI] Applying custom generationConfig for model ${modelIdToUse}`);
     try {
        currentGenerativeModel = vertex_ai.getGenerativeModel({
          model: modelIdToUse,
          generationConfig: {
            ...currentGenerativeModel.generationConfig, // ê¸°ì¡´ ëª¨ë¸ì˜ generationConfigë¥¼ ê¸°ë³¸ìœ¼ë¡œ
            ...options.generationConfig, // optionsì— ìžˆëŠ” ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
          },
        });
     } catch (error) {
        const errorMsg = `Failed to apply new generationConfig to Vertex AI model '${modelIdToUse}': ${error.message}`;
        logError("getVertexAiApiResponse:genConfig", new Error(errorMsg), { originalError: error });
        if (streamResponseCallback && typeof streamResponseCallback === "function") {
          streamResponseCallback(null, new Error(errorMsg));
          return null;
        }
        throw new Error(errorMsg);
     }
  }


  if (!currentGenerativeModel) {
    const errorMsg = `Vertex AI model '${modelIdToUse}' not available.`;
    logError("getVertexAiApiResponse:modelNotAvailable", new Error(errorMsg));
    if (streamResponseCallback && typeof streamResponseCallback === "function") {
      streamResponseCallback(null, new Error(errorMsg));
      return null;
    }
    throw new Error(errorMsg);
  }
  console.log(`[VertexAI] Using model: ${currentGenerativeModel.model}`);

  let conversationContents = [];
  let finalSystemMessageText = systemMessageText ? systemMessageText.trim() : "";

  if (specialModeType === "canvas") {
    const canvasSystemPrompt = "ì¤‘ìš”: ë‹¹ì‹ ì€ HTML, CSS, JavaScript ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” AIìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ìš”ì²­ì— ë”°ë¼ ì›¹ íŽ˜ì´ì§€ì˜ êµ¬ì¡°(HTML)ì™€ ìŠ¤íƒ€ì¼(CSS)ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ê° ì½”ë“œëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(ì˜ˆ: ```html ... ```, ```css ... ```)ìœ¼ë¡œ ê°ì‹¸ì„œ ì œê³µí•´ì£¼ì„¸ìš”. HTMLì—ëŠ” ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¥¼ í¬í•¨í•˜ê³ , CSSëŠ” í•´ë‹¹ HTMLì„ ìŠ¤íƒ€ì¼ë§í•˜ëŠ” ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. JavaScriptê°€ í•„ìš”í•˜ë‹¤ë©´ ê·¸ê²ƒë„ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìžê°€ 'ìº”ë²„ìŠ¤ì— ê·¸ë¦¼ ê·¸ë ¤ì¤˜' ê°™ì´ ëª¨í˜¸í•˜ê²Œ ìš”ì²­í•˜ë©´, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê·¸ë¦¼ì¸ì§€ ë˜ë¬»ê±°ë‚˜ ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ì œì‹œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ì½”ë“œëŠ” ë°”ë¡œ ì›¹íŽ˜ì´ì§€ì— ì ìš©ë  ìˆ˜ ìžˆë„ë¡ ì™„ì „í•œ í˜•íƒœë¡œ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.";
    finalSystemMessageText = finalSystemMessageText ? `${finalSystemMessageText}\n\n${canvasSystemPrompt}` : canvasSystemPrompt;
  } else if (specialModeType === "search") {
    const searchSystemPrompt = "Please answer based on web search results if necessary. Provide concise answers with relevant information found.";
    finalSystemMessageText = finalSystemMessageText ? `${finalSystemMessageText}\n${searchSystemPrompt}` : searchSystemPrompt;
  } else if (specialModeType === "chatbot") {
    const chatbotSystemPrompt = "ê³µì§€ì‚¬í•­/QnA ì—ëŸ¬í•´ê²°ìš© ì±—ë´‡ í”„ë¡¬í”„íŠ¸ - ì •í™•í•˜ê³  ì¹œì ˆí•œ ê¸°ìˆ  ì§€ì›ì„ ì œê³µí•˜ë©°, ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•ì„ ì•ˆë‚´í•´ì£¼ì„¸ìš”. ë¬¸ì œì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤.";
    finalSystemMessageText = finalSystemMessageText ? `${finalSystemMessageText}\n\n${chatbotSystemPrompt}` : chatbotSystemPrompt;
  }

  const systemInstruction = finalSystemMessageText && finalSystemMessageText.trim() !== ""
    ? { parts: [{ text: finalSystemMessageText.trim() }] }
    : null;

  conversationContents = [...history];
  
  // ðŸ”§ ì¤‘ë³µ ë°©ì§€: ëŒ€í™” ì´ë ¥ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ í˜„ìž¬ ì‚¬ìš©ìž ë©”ì‹œì§€ì™€ ê°™ìœ¼ë©´ ì œê±°
  if (conversationContents.length > 0) {
    const lastMsg = conversationContents[conversationContents.length - 1];
    if (lastMsg.role === "user" && 
        lastMsg.parts && 
        lastMsg.parts[0] && 
        lastMsg.parts[0].text === currentUserMessage) {
      console.log(`[VertexAI] ì¤‘ë³µëœ ì‚¬ìš©ìž ë©”ì‹œì§€ ë°œê²¬, ì´ë ¥ì—ì„œ ì œê±°: "${currentUserMessage.substring(0, 50)}..."`);
      conversationContents.pop(); // ë§ˆì§€ë§‰ ì¤‘ë³µ ë©”ì‹œì§€ ì œê±°
    }
  }
  
  // í˜„ìž¬ ì‚¬ìš©ìž ë©”ì‹œì§€ ì¶”ê°€
  conversationContents.push({ role: "user", parts: [{ text: currentUserMessage }] });

  // generationConfigë¥¼ currentGenerativeModelì—ì„œ ê°€ì ¸ì˜¤ê³ , options.max_output_tokens_overrideë¡œ ë®ì–´ì“°ê¸°
  const finalGenerationConfig = { ...currentGenerativeModel.generationConfig };
  if (options.max_output_tokens_override !== undefined) {
    const userMaxTokens = parseInt(options.max_output_tokens_override, 10);
    if (!isNaN(userMaxTokens) && userMaxTokens > 0) {
      finalGenerationConfig.maxOutputTokens = userMaxTokens;
      console.log(`[VertexAI] Overriding maxOutputTokens to: ${userMaxTokens}`);
    } else {
      logError("getVertexAiApiResponse:genConfig", new Error(`Invalid max_output_tokens_override: ${options.max_output_tokens_override}. Using default ${finalGenerationConfig.maxOutputTokens}.`));
    }
  }

  const request = {
    contents: conversationContents,
    systemInstruction: systemInstruction,
    generationConfig: finalGenerationConfig, // ìµœì¢…ì ìœ¼ë¡œ ê²°ì •ëœ generationConfig ì‚¬ìš©
  };

  console.log("[VertexAI] Request (final):", JSON.stringify(request, null, 2).substring(0, 500) + "...");


  try {
    if (streamResponseCallback && typeof streamResponseCallback === "function") {
      let streamErrorOccurred = false;
      const streamResult = await currentGenerativeModel.generateContentStream(request);
      for await (const item of streamResult.stream) {
        if (item && item.candidates && item.candidates[0] && item.candidates[0].content && item.candidates[0].content.parts && item.candidates[0].content.parts.length > 0) {
          const chunkText = item.candidates[0].content.parts.map((part) => part.text).join("");
          streamResponseCallback(chunkText);
        } else {
          const streamError = new Error("Malformed stream item from Vertex AI");
          logError("getVertexAiApiResponse:streamError", streamError, { item });
          streamResponseCallback(null, streamError);
          streamErrorOccurred = true;
          break;
        }
      }
      if (!streamErrorOccurred) {
        streamResponseCallback(null, null); // ìŠ¤íŠ¸ë¦¼ ì„±ê³µì  ì¢…ë£Œ ì•Œë¦¼
      }
      // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì˜ ê²½ìš°, ì „ì²´ contentë¥¼ ë°˜í™˜í•˜ì§€ ì•Šê³  ì½œë°±ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŒì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ null ë˜ëŠ” íŠ¹ì • ê°ì²´ ë°˜í™˜
      return { streaming_handled: true, model_used: currentGenerativeModel.model, provider: "vertexai" };
    } else {
      const result = await currentGenerativeModel.generateContent(request);
      if (result && result.response && result.response.candidates && result.response.candidates.length > 0 &&
          result.response.candidates[0].content && result.response.candidates[0].content.parts &&
          result.response.candidates[0].content.parts.length > 0) {
        const aiResponseText = result.response.candidates[0].content.parts.map((part) => part.text).join("");
        return {
          content: aiResponseText,
          actual_output_tokens: result.response.usageMetadata?.candidatesTokenCount || 0,
          input_tokens_processed: result.response.usageMetadata?.promptTokenCount || 0,
          model_used: currentGenerativeModel.model,
          provider: "vertexai",
        };
      } else {
        const errorMsg = "Invalid AI response structure from Vertex AI (non-streaming).";
        logError("getVertexAiApiResponse:invalidResponse", new Error(errorMsg), { response: result.response });
        throw new Error(errorMsg);
      }
    }
  } catch (error) {
    logError("getVertexAiApiResponse:apiCall", new Error(`Vertex AI API call failed: ${error.message}`), { originalError: error, requestDetails: { modelIdToUse, systemInstructionIsSet: !!systemInstruction } });
    if (error.message && error.message.includes("<!DOCTYPE")) {
      console.error("[VertexAI] HTML response received. This often indicates authentication or network issues.");
    }
    if (streamResponseCallback && typeof streamResponseCallback === "function") {
      streamResponseCallback(null, error);
      return { streaming_handled: true, error: true, model_used: modelIdToUse, provider: "vertexai" };
    } else {
      throw error; // Re-throw for non-streaming errors
    }
  }
}

module.exports = {
  getVertexAiApiResponse,
  // vertex_ai ë° generativeModelì€ ë‚´ë¶€ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³  ì§ì ‘ exportí•˜ì§€ ì•ŠìŒ
};
